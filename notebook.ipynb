{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from langchain.prompts import PromptTemplate\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "#환경설정\n",
    "ENV_PATH = './.env'\n",
    "CACHE_DIR = './.cache'\n",
    "FILES_DIR = os.path.join(CACHE_DIR, 'files')\n",
    "EMBEDDINGS_DIR = os.path.join(CACHE_DIR, 'embeddings')\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt 생성\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "chat_template =  ChatPromptTemplate.from_messages(\n",
    "  [\n",
    "    (\"system\", \"\"\"당신은 느린 학습자 관련 교육 전문가입니다. 당신은 느린 학습자 관련 질문을 받고, 이에 대한 전문적인 답변을 제공합니다.\n",
    "             아래 컨텍스트를 사용하여 질문에 답변하십시오. 답을 모르면 \"모르겠습니다.\"라고 답변하세요. 답을 지어내지 마세요.\n",
    "        컨텍스트: {context}\"\"\"),\n",
    "    (\"ai\", \"안녕하세요! 느린 학습자와 관련해서 어떤 사항이 궁금하신가요?\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "  ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM model\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache\n",
    "from langchain.callbacks import get_openai_callback #토큰 추적\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "\n",
    "openai_api_key = os.getenv(\"openai_api_key\")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", \n",
    "                  api_key=openai_api_key, \n",
    "                verbose=True,\n",
    "                  )\n",
    "\n",
    "#캐싱 : 인메모리 캐시를 사용. - 인메모리 vs. sqlite cache (추후 시간 비교해보기)\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "#토큰 사용량 추적\n",
    "# with get_openai_callback() as cb:\n",
    "#   result = llm.invoke(\"대한민국의 수도는 어디인가요?\")\n",
    "#   print(result)\n",
    "\n",
    "#대화 버퍼 메모리\n",
    "conversation = ConversationChain(\n",
    "  llm=llm,\n",
    "  memory = ConversationBufferWindowMemory(k=8),\n",
    "  verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, what's up?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm just here, ready to chat with you. How can I assist you today? We could talk about anything from the latest in technology, to your favorite movies, or even some fun facts. What's on your mind?\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, what's up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, what's up?\n",
      "AI: Hello! I'm just here, ready to chat with you. How can I assist you today? We could talk about anything from the latest in technology, to your favorite movies, or even some fun facts. What's on your mind?\n",
      "Human: What is the capital of Korea?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Korea is divided into two countries: North Korea and South Korea. The capital of South Korea is Seoul, a vibrant city known for its modern skyscrapers, high-tech subways, and pop culture. The capital of North Korea is Pyongyang, which is known for its monuments and grand architecture. Do you have any specific interest in either of these cities or countries?'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is the capital of Korea?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-12 16:40:19.218 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /root/.cache/pypoetry/virtualenvs/langchain-rag-T8wIn6co-py3.12/lib/python3.12/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-08-12 16:40:19.219 Session state does not function when running a script without `streamlit run`\n"
     ]
    }
   ],
   "source": [
    "#I. Streamlit 관련\n",
    "st.title('RAG 시스템')\n",
    "\n",
    "# 모델 선택 및 API Key 입력(필요시)\n",
    "st.subheader('모델 및 파라미터 설정')\n",
    "selected_model = st.sidebar.selectbox('LLM 모델을 선택하세요', ['Openai-GPT-4o', 'Google-Gemma-2-9b', 'Meta-Llama-3.1-8b'], key='selected_model')\n",
    "if selected_model == 'Openai-GPT-4o':\n",
    "    with st.sidebar:\n",
    "        openai_api_key = st.text_input('Openai API Key를 입력해주세요')\n",
    "\n",
    "# 사용자 입력\n",
    "if user_input := st.chat_input(\"질문을 입력하세요\"):\n",
    "  if not openai_api_key:\n",
    "    st.info(\"Openai API key를 입력해주세요\")\n",
    "    st.stop()\n",
    "#II. Langchain 관련\n",
    "#Langsmith 설정\n",
    "load_dotenv()\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"사내 LLM 구축 프로젝트\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1024)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedder\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name='BAAI/bge-m3',\n",
    "    model_kwargs={'device':'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings':True},\n",
    ")\n",
    "\n",
    "embeddings_model\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        '안녕하세요!',\n",
    "        '어! 오랜만이에요',\n",
    "        '이름이 어떻게 되세요?',\n",
    "        '날씨가 추워요',\n",
    "        'Hello LLM!'\n",
    "    ]\n",
    ")\n",
    "len(embeddings), len(embeddings[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loader\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "\n",
    "@st.cache_data(show_spinner=\"파일 로딩중...\")\n",
    "def load_file(file):\n",
    "  file_contents = file.read\n",
    "  file_path = os.path.join(FILES_DIR, file.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-rag-T8wIn6co-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
